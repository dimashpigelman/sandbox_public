{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "8wkV75Db9tXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "DSffnnWDNhb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA9bQMozM_wg",
        "outputId": "0c05960a-0040-4b2b-a0b5-55513cf113eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 22 19:34:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0             28W /   70W |    8126MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()"
      ],
      "metadata": {
        "id": "VFF-QRfXgpCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ],
      "metadata": {
        "id": "H7YQbFlINnGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install SAM2 and dependencies"
      ],
      "metadata": {
        "id": "yo5LAKqyNzfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd {HOME}/segment-anything-2\n",
        "!pip install -e . -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBrYxp6nNpqk",
        "outputId": "db00d1e6-3876-4ca3-bd92-76a35bcd03de"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'segment-anything-2' already exists and is not an empty directory.\n",
            "/content/segment-anything-2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/facebookresearch/sam2.git"
      ],
      "metadata": {
        "id": "xwcYSxQDfzOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision jupyter_bbox_widget"
      ],
      "metadata": {
        "id": "UGv98ypgPA7c"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download SAM2 checkpoints\n",
        "\n",
        "**NOTE:** SAM2 is available in 4 different model sizes ranging from the lightweight \"sam2_hiera_tiny\" (38.9M parameters) to the more powerful \"sam2_hiera_large\" (224.4M parameters)."
      ],
      "metadata": {
        "id": "g3Psmg3sOzIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints"
      ],
      "metadata": {
        "id": "Dq_DR0IJN_1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e742fdcf-f309-4759-9d82-0ea9970a8c80"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download example data\n",
        "\n",
        "**NONE:** Let's download few example images. Feel free to use your images or videos."
      ],
      "metadata": {
        "id": "7zPgcOxiQHiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg -P {HOME}/data"
      ],
      "metadata": {
        "id": "_p1zpkKZPvFu"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}"
      ],
      "metadata": {
        "id": "tVdiJQGYhC5P",
        "outputId": "0501a40b-812e-4817-8e39-77bc60f41d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "!cd GroundingDINO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN0FjiHgMkq6",
        "outputId": "ad6348a1-10c4-407c-c7b7-435d1f0e02fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroundingDINO'...\n",
            "remote: Enumerating objects: 463, done.\u001b[K\n",
            "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 463 (delta 175), reused 135 (delta 135), pack-reused 223 (from 1)\u001b[K\n",
            "Receiving objects: 100% (463/463), 12.87 MiB | 17.76 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "BHRLQPV4WKd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import supervision as sv\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor"
      ],
      "metadata": {
        "id": "q-HSJrPgQLsK"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Florence utilities (from utils/florence.py) ---\n",
        "FLORENCE_CHECKPOINT = \"microsoft/Florence-2-base\"\n",
        "FLORENCE_OPEN_VOCABULARY_DETECTION_TASK = \"<OPEN_VOCABULARY_DETECTION>\"\n",
        "\n",
        "def load_florence_model(device: torch.device):\n",
        "    \"\"\"Load Florence-2 model & processor.\"\"\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        FLORENCE_CHECKPOINT, trust_remote_code=True\n",
        "    ).to(device).eval()\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        FLORENCE_CHECKPOINT, trust_remote_code=True\n",
        "    )\n",
        "    return model, processor\n",
        "\n",
        "def run_florence_inference(model, processor, device, image: Image.Image, text: str):\n",
        "    \"\"\"Run Florence on image + text prompt.\"\"\"\n",
        "    prompt = FLORENCE_OPEN_VOCABULARY_DETECTION_TASK + text\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        num_beams=3\n",
        "    )\n",
        "    gen = processor.batch_decode(outputs, skip_special_tokens=False)[0]\n",
        "    # post_process_generation returns a dict with 'boxes' and 'labels'\n",
        "    resp = processor.post_process_generation(gen,\n",
        "                task=FLORENCE_OPEN_VOCABULARY_DETECTION_TASK,\n",
        "                image_size=image.size)\n",
        "    return resp  # e.g. {\"boxes\": [...], \"labels\": [...]}\n",
        "# :contentReference[oaicite:0]{index=0}\n",
        "\n",
        "# --- SAM 2 utilities (adapted from utils/sam.py) ---\n",
        "SAM_CONFIG = \"./configs/sam2/sam2_hiera_l.yaml\"\n",
        "SAM_CHECKPOINT = \"./checkpoints/sam2_hiera_large.pt\"\n",
        "\n",
        "def load_sam_image_model(device: torch.device):\n",
        "    \"\"\"Build and wrap the hiera_large SAM2 model.\"\"\"\n",
        "    sam_model = build_sam2(SAM_CONFIG, SAM_CHECKPOINT, device=device)\n",
        "    return SAM2ImagePredictor(sam_model)\n",
        "\n",
        "def run_sam_inference(predictor: SAM2ImagePredictor,\n",
        "                      image: Image.Image,\n",
        "                      boxes: np.ndarray):\n",
        "    \"\"\"Run SAM2 on detected boxes to produce masks.\"\"\"\n",
        "    rgb = np.array(image.convert(\"RGB\"))\n",
        "    predictor.set_image(rgb)\n",
        "    # single-mask per box\n",
        "    masks, scores, _ = predictor.predict(box=boxes, multimask_output=False)\n",
        "    # masks: (N, H, W)\n",
        "    return masks\n",
        "# :contentReference[oaicite:1]{index=1}"
      ],
      "metadata": {
        "id": "Sms2esO7QK1j"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "IMAGE_PATH = f\"{HOME}/data/1.jpeg\"\n",
        "prompt = \"find the ruler\"\n",
        "out_path = f\"{HOME}/data/1_ruler.jpeg\""
      ],
      "metadata": {
        "id": "lLHLCQN3QmV8"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# load models\n",
        "flor_model, flor_proc = load_florence_model(device)\n",
        "sam_predictor   = load_sam_image_model(device)"
      ],
      "metadata": {
        "id": "nJz9CwU5SakW"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load image\n",
        "image_path = IMAGE_PATH\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "text_prompt = prompt\n",
        "# run Florence for each comma-separated text\n",
        "texts = [t.strip() for t in text_prompt.split(\",\") if t.strip()]\n",
        "all_boxes, all_labels = [], []"
      ],
      "metadata": {
        "id": "-dJS13KiTBVZ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for txt in texts:\n",
        "    resp = run_florence_inference(flor_model, flor_proc, device, image, txt)\n",
        "    # resp[\"boxes\"]: List of [x1,y1,x2,y2], resp[\"labels\"]: list of ints\n",
        "    all_boxes.extend(resp['<OPEN_VOCABULARY_DETECTION>']['bboxes'])\n",
        "    all_labels.extend(resp['<OPEN_VOCABULARY_DETECTION>']['bboxes_labels'])\n",
        "\n",
        "if not all_boxes:\n",
        "    print(f\"No regions found for prompt: '{text_prompt}'\")\n",
        "\n",
        "boxes = np.array(all_boxes)\n",
        "labels= all_labels"
      ],
      "metadata": {
        "id": "aldITpP_SjAW"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run SAM on all detected boxes\n",
        "masks = run_sam_inference(sam_predictor, image, boxes)\n",
        "\n"
      ],
      "metadata": {
        "id": "U83smuiTSixS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate(image: Image.Image, boxes: np.ndarray, masks: np.ndarray, labels: list[str]):\n",
        "    \"\"\"\n",
        "    Overlay masks, boxes, and text labels on the image.\n",
        "    - boxes: (N,4) array of [x1,y1,x2,y2]\n",
        "    - masks: (N,H,W) boolean array of masks\n",
        "    - labels: list of N human-readable strings\n",
        "    \"\"\"\n",
        "    scene = np.array(image.convert(\"RGB\"))\n",
        "    masks = masks.astype(bool)\n",
        "    # Create a Detections object\n",
        "    det = sv.Detections(\n",
        "        xyxy     = boxes,\n",
        "        mask     = masks,\n",
        "        class_id = np.arange(len(labels))  # dummy IDs\n",
        "    )\n",
        "\n",
        "    # Use INDEX color-mapping so each detection gets its own color\n",
        "    mask_annotator  = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "    box_annotator   = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "    label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "    # Draw masks, boxes, and finally the text labels\n",
        "    scene = mask_annotator.annotate(scene=scene, detections=det)\n",
        "    scene = box_annotator.annotate(scene=scene, detections=det)\n",
        "    scene = label_annotator.annotate(\n",
        "        scene      = scene,\n",
        "        detections = det,\n",
        "        labels     = labels\n",
        "    )\n",
        "\n",
        "    return Image.fromarray(scene)\n"
      ],
      "metadata": {
        "id": "R99cGYjoWxGY"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# annotate and save\n",
        "annotated = annotate(image, boxes, masks, labels)\n",
        "output_path = out_path\n",
        "annotated.save(output_path)\n",
        "print(f\"Result saved to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JByPaNMVO2E",
        "outputId": "93c3c6a8-e039-4c75-8f8a-1df822e517d7"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result saved to /content/data/1_ruler.jpeg\n"
          ]
        }
      ]
    }
  ]
}